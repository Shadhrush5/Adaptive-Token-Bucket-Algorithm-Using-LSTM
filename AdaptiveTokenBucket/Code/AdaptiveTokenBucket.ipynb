{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shadhrushswaroop/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([20802, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m     outputs \u001b[39m=\u001b[39m model(flow_rates[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m))\n\u001b[1;32m     57\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, flow_rates[\u001b[39m1\u001b[39m:])\n\u001b[0;32m---> 58\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     59\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     61\u001b[0m \u001b[39m# Evaluation of the token bucket model\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        out = self.fc(h[-1])\n",
    "        return out\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"../DataSet/cs448b_ipasn.csv\")\n",
    "\n",
    "# Extract the required columns\n",
    "dates = pd.to_datetime(data[\"date\"])\n",
    "local_ips = data[\"l_ipn\"]\n",
    "remote_asns = data[\"r_asn\"]\n",
    "flow_counts = data[\"f\"]\n",
    "\n",
    "# Token bucket parameters\n",
    "capacity = 1000\n",
    "tokens = capacity\n",
    "rate = 100  # tokens per second\n",
    "\n",
    "# LSTM parameters\n",
    "input_size = 1\n",
    "hidden_size = 32\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Prepare the flow rate data for LSTM training\n",
    "flow_rates = flow_counts.values.reshape(-1, 1)\n",
    "scaler = MinMaxScaler()\n",
    "flow_rates = scaler.fit_transform(flow_rates)\n",
    "flow_rates = torch.FloatTensor(flow_rates)\n",
    "\n",
    "# Train the LSTM model\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(flow_rates[:-1].unsqueeze(2))\n",
    "    loss = criterion(outputs, flow_rates[1:])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation of the token bucket model\n",
    "total_tokens = 0\n",
    "total_flow_count = 0\n",
    "total_matched_flow_count = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    current_time = time.time()\n",
    "    flow_count = flow_counts[i]\n",
    "    token_requirement = flow_count\n",
    "\n",
    "    # Wait until tokens are available\n",
    "    while tokens < token_requirement:\n",
    "        time.sleep(0.1)\n",
    "        elapsed_time = time.time() - current_time\n",
    "        tokens += elapsed_time * rate\n",
    "\n",
    "        # Adjust tokens based on the flow rate (adaptive component using LSTM)\n",
    "        flow_rate_input = torch.FloatTensor([[flow_count]])\n",
    "        predicted_flow_rate = model(flow_rate_input.unsqueeze(2)).item()\n",
    "        token_requirement = int(predicted_flow_rate * elapsed_time)\n",
    "\n",
    "        # Ensure tokens don't exceed the capacity\n",
    "        if tokens > capacity:\n",
    "            tokens = capacity\n",
    "\n",
    "    # Tokens are available, process the flow\n",
    "    tokens -= token_requirement\n",
    "\n",
    "    # Evaluation\n",
    "    total_tokens += token_requirement\n",
    "    total_flow_count += flow_count\n",
    "    if token_requirement == flow_count:\n",
    "        total_matched_flow_count += 1\n",
    "\n",
    "    # Print information about the flow\n",
    "    # date = dates[i].strftime(\"%Y-%m-%d\")\n",
    "    # local_ip = local_ips[i]\n",
    "    # remote_asn = remote_asns[i]\n",
    "    # print(f\"Flow on {date}: Local IP={local_ip}, Remote ASN={remote_asn}, Flow Count={flow_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Token Utilization:2.489929337114839\n",
      "Accuracy:0.8139947934634729\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "token_utilization = (total_tokens / (rate * len(data))) * 100\n",
    "accuracy = (total_matched_flow_count / total_flow_count) * 100\n",
    "\n",
    "print(f\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Token Utilization:{token_utilization}\")\n",
    "print(f\"Accuracy:{accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
